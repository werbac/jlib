{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>First Steps - Random Forrest Script - https://www.kaggle.com/c/street-view-getting-started-with-julia/details/julia-tutorial</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_data (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Images, DataFrames, Colors,  HDF5, DecisionTree  \n",
    "\n",
    "wd=\"/media/u01/analytics/scoring/k/FirstSteps/\";   \n",
    "\n",
    "#typeData could be either \"train\" or \"test. labelsInfo should contain the IDs of each image to be read\n",
    "#The images in the trainResized and testResized data files are 20x20 pixels, so imageSize is set to 400.\n",
    "#path should be set to the location of the data files.\n",
    "\n",
    "function read_data(typeData, labelsInfo, imageSize, path)\n",
    "    x = zeros(size(labelsInfo, 1), imageSize)   #Intialize x matrix\n",
    "    for (index, idImage) in enumerate(labelsInfo[:ID])    \n",
    "        nameFile = \"$(path)/$(typeData)Resized/$(idImage).Bmp\"\n",
    "        img = imread(nameFile)\n",
    "        temp = convert(Image{Gray}, img)\n",
    "        #temp = float32sc(img)\n",
    "        #if ndims(temp) == 3       #Convert color images to gray images by taking the average of the color scales. \n",
    "        #    temp = mean(temp.data, 1)\n",
    "        #end\n",
    "        x[index, :] = reshape(temp, 1, imageSize)    #Transform image matrix to a vector and store it in data matrix \n",
    "     end \n",
    "     return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: `getindex` has no method matching getindex(::DataFrames.DataFrame, ::ASCIIString)\nClosest candidates are:\n  getindex(::DataFrames.DataFrame, !Matched::Real, !Matched::Union{Real,Symbol})\n  getindex{T<:Union{Real,Symbol}}(::DataFrames.DataFrame, !Matched::Real, !Matched::AbstractArray{T<:Union{Real,Symbol},1})\n  getindex(::DataFrames.DataFrame, !Matched::Real, !Matched::Colon)\n  ...\nwhile loading In[8], in expression starting on line 4",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: `getindex` has no method matching getindex(::DataFrames.DataFrame, ::ASCIIString)\nClosest candidates are:\n  getindex(::DataFrames.DataFrame, !Matched::Real, !Matched::Union{Real,Symbol})\n  getindex{T<:Union{Real,Symbol}}(::DataFrames.DataFrame, !Matched::Real, !Matched::AbstractArray{T<:Union{Real,Symbol},1})\n  getindex(::DataFrames.DataFrame, !Matched::Real, !Matched::Colon)\n  ...\nwhile loading In[8], in expression starting on line 4",
      "",
      " in read_data at In[7]:7"
     ]
    }
   ],
   "source": [
    "imageSize = 400      # 20 x 20 pixel\n",
    "labelsInfoTrain = readtable(\"$(wd)/trainLabels.csv\")\n",
    "xTrain = read_data(\"train\", labelsInfoTrain, imageSize, wd)     #Read training matrix\n",
    "labelsInfoTest = readtable(\"$(wd)/sampleSubmission.csv\")\n",
    "xTest = read_data(\"test\", labelsInfoTest, imageSize, wd)\n",
    "\n",
    "#Get only first character of string (convert from string to character).\n",
    "#Apply the function to each element of the column \"Class\"\n",
    "yTrain = map(x -> x[1], labelsInfoTrain[:Class])\n",
    "\n",
    "#Convert from character to integer\n",
    "yTrain = int(yTrain)\n",
    "\n",
    "#Train random forest with 20 for number of features chosen at each random split,\n",
    "#50 for number of trees, and 1.0 for ratio of subsampling.\n",
    "model = build_forest(yTrain, xTrain, 20, 50, 1.0)\n",
    "predTest = apply_forest(model, xTest)  #Get predictions for test data\n",
    "labelsInfoTest[:Class] = char(predTest)  #Convert integer predictions to character\n",
    "#writetable(\"$(path)/juliaSubmission.csv\", labelsInfoTest, separator=',', header=true)  #Save predictions\n",
    "accuracy = nfoldCV_forest(yTrain, xTrain, 20, 50, 4, 1.0);\n",
    "println(\"4 fold accuracy: $(mean(accuracy))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxrwxr-x. 3 rmadmin rmadmin 4096 May 30 12:45 ..\n",
      "drwxr-xr-x. 2 rmadmin rmadmin 4096 May 30 12:46 .ipynb_checkpoints\n",
      "-rw-------. 1 rmadmin rmadmin 3223 May 30 14:00 nohup.out\n",
      "-rw-rw-r--. 1 rmadmin rmadmin 5762 May 30 14:00 Julia-Kaggle0.ipynb\n",
      "drwxrwxrwx. 3 rmadmin rmadmin 4096 May 30 14:00 .\n"
     ]
    }
   ],
   "source": [
    "run(`ls -lart`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Santander - XGBoost - https://www.kaggle.com/aeithne/santander-customer-satisfaction/palmares/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using DataFrames, BinDeps, XGBoost\n",
    "now = time()\n",
    "Patterns = readtable(\"/media/u01/analytics/scoring/k/Santander/train.csv\")\n",
    "println(\"Start:\", time() - now);\n",
    "Test = readtable(\"/media/u01/analytics/scoring/k/Santander/test.csv\")\n",
    "idList = Test[:ID]\n",
    "\n",
    "println(\"Training = \", size(Patterns))\n",
    "println(\"Test = \", size(Test))\n",
    "Patterns[Patterns[:var3] .== -999999, :var3] = 2;\n",
    "\n",
    "trainx, trainy = size(Patterns[:,1:370])\n",
    "testx, testy = size(Test[:,1:370])\n",
    "\n",
    "for i in 1:trainx\n",
    "    Patterns[:var38][i] = log(Patterns[:var38][i])\n",
    "end    \n",
    "for i in 1:testx\n",
    "    Test[:var38][i] = log(Test[:var38][i])\n",
    "end\n",
    "\n",
    "#showcols(Patterns)\n",
    "xg_trainx = sparse(Array{Float64}(Patterns[2:370]))\n",
    "xg_trainy = Array{Float64}(Patterns[:TARGET])\n",
    "xg_testx = sparse(Array{Float64}(Test[2:370]))\n",
    "xg_train = DMatrix(xg_trainx, label = xg_trainy)\n",
    "xg_test = DMatrix(xg_testx)\n",
    "#Patterns2 = readtable(\"./train_patterns.csv\")\n",
    "num_round = 560\n",
    "max_depth = 5\n",
    "\n",
    "dtrain = xg_train\n",
    "dtest = xg_test\n",
    "\n",
    "bst = xgboost(dtrain, num_round, eta = 0.0202048, booster = \"gbtree\", colsample_bytree    = 0.701, subsample           = 0.6815, metrics=[\"auc\", \"error\"], max_depth = max_depth)\n",
    "preds = XGBoost.predict(bst, xg_testx)\n",
    "#firstload = false;\n",
    "for v in 1:length(preds)\n",
    "    nv = Test[:num_var33][v]+Test[:saldo_medio_var33_ult3][v]+Test[:saldo_medio_var44_hace2][v]+Test[:saldo_medio_var44_hace3][v]+\n",
    "    Test[:saldo_medio_var33_ult1][v]+Test[:saldo_medio_var44_ult1][v]\n",
    "    if nv > 0\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:var15][v] < 23\n",
    "        preds[v] = 0\n",
    "    elseif (Test[:var15][v] > 102)\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_medio_var5_hace2][v] > 160000\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_var33][v] > 0\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:var38][v] > log(3988596)\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:var21][v] > 7500\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:num_var30][v] > 9\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:num_var13_0][v] > 6\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:num_var33_0][v] > 0\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:imp_ent_var16_ult1][v] > 51003\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:imp_op_var39_comer_ult3][v] > 13184\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_medio_var5_ult3][v] > 108251\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:num_var37_0][v] > 45\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_var5][v] > 137615\n",
    "        preds[v] = 0\n",
    "        end\n",
    "    if Test[:saldo_var8][v] > 60099\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if (Test[:var15][v]+Test[:num_var45_hace3][v]+Test[:num_var45_ult3][v]+Test[:var36][v]) <= 24\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_var14][v] > 19053.78\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_var17][v] > 288188.97\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:saldo_var26][v] > 10381.29\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:num_var13_largo_0][v] > 3\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    if Test[:imp_op_var40_comer_ult1][v] > 3639.87\n",
    "        preds[v] = 0\n",
    "    end\n",
    "    \n",
    "    # if (Test[:var15][v] <= 22)\n",
    "        # preds[v] = 0\n",
    "    # \n",
    "    \n",
    "    # if Test[:saldo_var5][v] > 137615\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    \n",
    "    # if Test[:saldo_medio_var5_hace2][v] > 160000\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    \n",
    "    if Test[:var3][v] in [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,68,69,71,72,73,74,76,77,78,79,81,82,84,85,86,87,88,89,90,91,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,110,112,115,117,118,119,120,121,122,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,141,143,144,145,146,147,148,149,150,151,152,153,154,156,157,158,159,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,204,205,207,208,209,210,211,213,215,216,217,218,219,220,223,225,228,229,231,235,238]\n",
    "       preds[v] = 0\n",
    "    end\n",
    "    \n",
    "    # if Test[:var33][v] > 0\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    # if Test[:var36][v] == 0\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    # if Test[:var36][v] > 3988596\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    # if Test[:var21][v] > 7500\n",
    "        # preds[v] = 0\n",
    "    # end\n",
    "    if preds[v] < 0\n",
    "        preds[v] = 0\n",
    "    elseif preds[v] > 1\n",
    "        preds[v] = 1\n",
    "    end\n",
    "    preds[v] = sqrt(preds[v])\n",
    "end\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "submission = DataFrame(ID = idList, TARGET = preds)\n",
    "writetable(\"juliaBenchmark.csv\", submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Rossmann-store-sales - XGBoost - https://www.kaggle.com/wacaxx/rossmann-store-sales/julia-xgboost-starter-code/code</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using BinDeps, DataFrames, XGBoost\n",
    "train = readtable(\"/media/u01/analytics/scoring/k/Ross/train.csv\")\n",
    "test = readtable(\"/media/u01/analytics/scoring/k/Ross/test.csv\")\n",
    "store = readtable(\"/media/u01/analytics/scoring/k/Ross/store.csv\")\n",
    "sample_submission = readtable(\"/media/u01/analytics/scoring/k/Ross/sample_submission.csv\")\n",
    "train = train[train[:Open] .== 1, :]   #Reduce the training set to ignore the closed stores\n",
    "train = join(train, store, on = :Store, kind = :inner)  #Tables inner join\n",
    "test = join(test, store, on = :Store, kind = :inner)\n",
    "tesdIdx = test[:Id]  #Test Ids & closed stores indexes \n",
    "closedStoreIdx = find(test[:Open] .== 0)\n",
    "#Remove NAs from competition distance data\n",
    "train[isna(train[:CompetitionDistance]), :CompetitionDistance] = 20000\n",
    "test[isna(test[:CompetitionDistance]), :CompetitionDistance] = 20000\n",
    "#accumulatedNAInfo = colwise(isna, train)\n",
    "#numberOfNAsPerColumn = map((x) -> sum(x) / length(x), accumulatedNAInfo)\n",
    "\n",
    "#Date transformation to \"Date\" object\n",
    "train[:Date] = Date(train[:Date], \"y-m-d\")\n",
    "test[:Date] = Date(test[:Date], \"y-m-d\")\n",
    "\n",
    "#Make new date related features\n",
    "train[:year] = year(train[:Date])\n",
    "train[:month] = month(train[:Date])\n",
    "train[:day] = day(train[:Date])\n",
    "test[:year] = year(test[:Date])\n",
    "test[:month] = month(test[:Date])\n",
    "test[:day] = day(test[:Date])\n",
    "\n",
    "#Define Categorical and Numeric Columns\n",
    "colummTypes = map(x -> string(x), eltypes(train))\n",
    "categoricalColumns = names(train)[colummTypes .== \"UTF8String\"]\n",
    "categoricalColumns = categoricalColumns[1:3] #Exclude promo interval\n",
    "numericalColumns = [:DayOfWeek, :Promo, :SchoolHoliday, :CompetitionDistance, :Promo2, :year, :month, :day]\n",
    "\n",
    "#Helper Function - This functions counts the occurences of a category and maps it back to the data provided\n",
    "function categorical2frequency(dataVector, vectorDict)\n",
    "  newVector = [vectorDict[dataPoint] for dataPoint in dataVector]\n",
    "  return newVector\n",
    "end\n",
    "\n",
    "#Categorical Columns to Frequency (Counts)\n",
    "for singleCol in categoricalColumns\n",
    "  #singleCol = categoricalColumns[1]\n",
    "  #countmap is similar to the function \"table()\" in R, the only difference is that countmap() returns a dictionary\n",
    "  vectorDict = countmap(train[singleCol])\n",
    "  #Apply counts to factors\n",
    "  train[singleCol] = categorical2frequency(train[singleCol], vectorDict)\n",
    "  test[singleCol] = categorical2frequency(test[singleCol], vectorDict)\n",
    "  #Print progress\n",
    "  print(string(singleCol) * \" Column Processed\")\n",
    "end\n",
    "\n",
    "#Define target\n",
    "costTrainingLog = convert(Array{Float32}, train[:Sales])\n",
    "\n",
    "#Transform data to XGBoost matrices\n",
    "trainArray = convert(Array{Float32},  train[:, vcat(numericalColumns, categoricalColumns)])\n",
    "testArray = convert(Array{Float32}, test[:, vcat(numericalColumns, categoricalColumns)])\n",
    "dtrain = DMatrix(trainArray, label = costTrainingLog)\n",
    "dtest = DMatrix(testArray)\n",
    "\n",
    "#XGBoost training\n",
    "num_round = 250\n",
    "param = [\"eta\" => 0.2, \"max_depth\" => 20, \"objective\" => \"reg:linear\", \"silent\" => 1]\n",
    "XGBoostModel = xgboost(dtrain, num_round, param = param)\n",
    "\n",
    "preds = predict(XGBoostModel, dtest)    #Predictions using test data\n",
    "preds[closedStoreIdx] = 0    #Round to zero closed stores\n",
    "sampleSubmission = DataFrame(Id = tesdIdx, Sales = preds)\n",
    "#writetable(\"juliaBenchmark.csv\", sampleSubmission)   #Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Cern - RandomForest -  https://www.kaggle.com/gomiero/flavours-of-physics/random-forest-in-julia </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# /media/u01/analytics/scoring/k/cern\n",
    "println(\"spwan 8 process to speed up train...\")\n",
    "addprocs(8)\n",
    "using DataFrames, DecisionTree\n",
    "\n",
    "# Agreement and correlation conditions\n",
    "const ks_cutoff  = 0.09\n",
    "const cvm_cutoff = 0.002\n",
    "\n",
    "function the_roc(p_labels::Array{Bool, 1}, p_data::Array{Float64, 1}, p_weights::Array{Float64, 1})\n",
    "    local score_index_ordered = sortperm(p_data, rev=true)\n",
    "    local labels_bool = p_labels[score_index_ordered]\n",
    "    local data_ordered = p_data[score_index_ordered]\n",
    "    local weights_ordered = p_weights[score_index_ordered]\n",
    "    # Find threshold\n",
    "    local distinct_value_indices = find(convert(Array{Bool, 1}, abs(diff(data_ordered)) .> 1.0e-08))\n",
    "    local threshold_idxs = [distinct_value_indices; length(labels_bool)]\n",
    "    # Accumulate true positives\n",
    "    local tps = copy(weights_ordered)\n",
    "    tps[find(!labels_bool)] = 0.0\n",
    "    tps = cumsum(tps)[threshold_idxs]\n",
    "    local fps = cumsum(weights_ordered)[threshold_idxs] .- tps\n",
    "    thresholds = data_ordered[threshold_idxs]\n",
    "    if length(tps) == 0 || fps[1] == 0.0\n",
    "        tps = [0.0; tps]\n",
    "        fps = [0.0; fps]\n",
    "        thresholds = [thresholds[1] + 1.0; thresholds]\n",
    "    end\n",
    "    if fps[end] <= 0.0\n",
    "        error(\"No negative samples in y_true, false positive value should be meaningless\")\n",
    "    else\n",
    "        fpr = fps ./ fps[end]\n",
    "    end\n",
    "    if tps[end] <= 0.0\n",
    "        error(\"No positive samples in y_true, true positive value should be meaningless\")\n",
    "    else\n",
    "        tpr = tps ./ tps[end]\n",
    "    end\n",
    "    return fpr, tpr, thresholds\n",
    "end\n",
    "\n",
    "             \n",
    "# Compute roc curve ; # data_zero: 0-labeled data ; data_one:  1-labeled data\n",
    "# sample_weights_zero: weights for 0-labeled data ; sample_weights_one:  weights for 1-labeled data ; return: roc curve\n",
    "function _roc_curve_splitted(data_zero::Array{Float64,1}, data_one::Array{Float64,1}, sample_weights_zero::Array{Float64,1},\n",
    "                             sample_weights_one::Array{Float64,1})\n",
    "    local labels = convert(Array{Bool, 1}, [zeros(data_zero); ones(data_one)])\n",
    "    local data_all = [data_zero; data_one]\n",
    "    local weights = [sample_weights_zero; sample_weights_one]\n",
    "\n",
    "    fpr, tpr = the_roc(labels, data_all, weights)\n",
    "\n",
    "    return fpr, tpr\n",
    "end\n",
    "                    \n",
    "# Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n",
    "# data_prediction: array-like, real data predictions ;  mc_prediction: array-like, Monte Carlo data predictions\n",
    "# weights_data: array-like, real data weights; weights_mc: array-like, Monte Carlo weights ; returns: ks value\n",
    "function compute_ks(p_data_prediction::Array{Float64,1}, p_mc_prediction::Array{Float64,1}, p_weights_data::Array{Float64,1},\n",
    "                    p_weights_mc::Array{Float64,1})\n",
    "    @assert length(p_data_prediction) == length(p_weights_data)\n",
    "    @assert length(p_mc_prediction) == length(p_weights_mc)\n",
    "    @assert reduce(&, p_data_prediction[:,1] .>= 0.0) && reduce(&, p_data_prediction[:,1] .<= 1.0)\n",
    "    @assert reduce(&, p_mc_prediction[:,1] .>= 0.0) && reduce(&, p_mc_prediction[:,1] .<= 1.0)\n",
    "    p_weights_data = p_weights_data ./ sum(p_weights_data)\n",
    "    p_weights_mc = p_weights_mc ./ sum(p_weights_mc)\n",
    "    fpr, tpr = _roc_curve_splitted(p_data_prediction, p_mc_prediction, p_weights_data, p_weights_mc)\n",
    "    Dnm = maximum(abs(fpr .- tpr))\n",
    "    return Dnm\n",
    "end\n",
    "\n",
    "\n",
    "# Rolling window: take window with definite size through the array\n",
    "# data: array-like; window_size: size; return: the sequence of windows\n",
    "# Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4  ---- Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n",
    "function _rolling_window(p_data::Array{Int64, 1}, p_window_size::Int64)\n",
    "    return [ p_data[i:i+p_window_size-1] for i in 1:length(p_data)-p_window_size ]\n",
    "end\n",
    "\n",
    "function _bincount(p_data::Array{Int64, 1}; p_weights=0, p_minlength=0)\n",
    "    sz = maximum(p_data) + 1\n",
    "    sz = ifelse(p_minlength > sz, p_minlength, sz)\n",
    "    ret = zeros(Int64, sz)\n",
    "    for i in p_data\n",
    "        idx = i + 1\n",
    "        ret[idx] = ret[idx] + 1\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "# Compute Cramer-von Mises metric. Compared two distributions, where first is subset of second one. Assuming that second is ordered by ascending\n",
    "# subindices: indices of events which will be associated with the first distribution\n",
    "# total_events: count of events in the second distribution := return: cvm metric\n",
    "function _cvm(subindices, total_events::Int64)\n",
    "    local target_distribution = collect(1:total_events) ./ total_events\n",
    "    local subarray_distribution = cumsum(_bincount(subindices, p_minlength=total_events))\n",
    "    subarray_distribution ./= subarray_distribution[end]\n",
    "    return mean((target_distribution .- subarray_distribution) .^ 2)\n",
    "end\n",
    "\n",
    "                     \n",
    "# Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n",
    "# In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n",
    "#predictions: array-like, predictions\n",
    "#masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n",
    "#n_neighbours: count of neighbours for event to define mass bin\n",
    "#step: step through sorted mass-array to define next center of bin\n",
    "#returns: average cvm value\n",
    "function compute_cvm(p_predictions::Array{Float64,1}, p_masses::Array{Float64,1}, n_neighbours=200, step=50)\n",
    "    @assert length(p_predictions) == length(p_masses)\n",
    "    # First, reorder by masses\n",
    "    local predictions_by_mass = p_predictions[sortperm(p_masses)]\n",
    "    # Second, replace probabilities with order of probability among other events\n",
    "    local predictions_ordered = sortperm(sortperm(predictions_by_mass))\n",
    "    # Now, each window forms a group, and we can compute contribution of each group to CvM\n",
    "    rw = _rolling_window(predictions_ordered, n_neighbours)\n",
    "    cvms = Array(Float64, 1)\n",
    "    for window in rw[1:end:step]\n",
    "        push!(cvms, _cvm(window, length(predictions_ordered)))\n",
    "    end\n",
    "    return mean(cvms)\n",
    "end\n",
    "\n",
    "# ----- START ------\n",
    "\n",
    "train = readtable(\"/media/u01/analytics/scoring/k/cern/training.csv\", quotemark=Char[])\n",
    "#features = convert(Array, train[:,[:LifeTime, :FlightDistance, :pt, :isolationa]])\n",
    "features = convert(Array{Float64, 2}, train[:,2:31])\n",
    "println(\"Features $(names(train[:,2:25]))\")\n",
    "labels = convert(Array{Float64,1}, train[:signal])\n",
    "model = build_forest(labels, features, 22, 50, 1.0)   # Random Forest\n",
    "#show(STDOUT, model)\n",
    "\n",
    "# Agreement Test\n",
    "check_agreement = readtable(\"/media/u01/analytics/scoring/k/cern/check_agreement.csv\", quotemark=Char[])\n",
    "features = convert(Array{Float64, 2}, check_agreement[:,2:31])\n",
    "agreement_probs = apply_forest(model, features)\n",
    "ks = compute_ks(agreement_probs[find(check_agreement[:signal] .==  0)], agreement_probs[find(check_agreement[:signal] .==  1)],\n",
    "                check_agreement[find(check_agreement[:signal] .==  0),:weight].data, check_agreement[find(check_agreement[:signal] .==  1),:weight].data\n",
    "               )\n",
    "if ks < ks_cutoff\n",
    "    println(\"The model passed the agreement test with $ks < $ks_cutoff\")\n",
    "else\n",
    "    println(\"The model failed the agreement test with $ks >= $ks_cutoff\")\n",
    "    #exit(1)\n",
    "end\n",
    "\n",
    "# Correlation Test\n",
    "check_correlation = readtable(\"/media/u01/analytics/scoring/k/cern/check_correlation.csv\", quotemark=Char[])\n",
    "features = convert(Array{Float64, 2}, check_correlation[:,2:31])\n",
    "correlation_probs = apply_forest(model, features)\n",
    "cvm = compute_cvm(correlation_probs, check_correlation[:mass].data)\n",
    "if cvm < cvm_cutoff\n",
    "    println(\"The model passed the correlation test with $cvm < $cvm_cutoff.\")\n",
    "else\n",
    "    println(\"The model failed the correlation test with $cvm >= $cvm_cutoff.\")\n",
    "    #exit(2)\n",
    "end\n",
    "\n",
    "# Make predictions\n",
    "test = readtable(\"/media/u01/analytics/scoring/k/cern/test.csv\")\n",
    "features = convert(Array, test[:,2:31])\n",
    "predictions = apply_forest(model, features)\n",
    "test[:prediction] = predictions\n",
    "#writetable(\"jsubmission.csv\", test[:,[:id,:prediction]], separator=',', header=true)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
